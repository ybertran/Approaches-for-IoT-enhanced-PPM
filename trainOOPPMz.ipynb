{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# torch packages\n",
    "import torch.nn as nn\n",
    "import unicodecsv\n",
    "from pandas.api.types import is_string_dtype\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "from collections import OrderedDict\n",
    "\n",
    "from LSTM import Model\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def generate_prefix_data(data, min_length, max_length):\n",
    "    # generate prefix data (each possible prefix becomes a trace)\n",
    "\n",
    "    case_length = data.groupby(case_id_col)[activity_col].transform(len)\n",
    "    data.loc[:, 'case_length'] = case_length.copy()\n",
    "    dt_prefixes = data[data['case_length'] >= min_length].groupby(case_id_col).head(min_length)\n",
    "    dt_prefixes[\"prefix_nr\"] = 1\n",
    "    dt_prefixes[\"orig_case_id\"] = dt_prefixes[case_id_col]\n",
    "    for nr_events in range(min_length, max_length+1):\n",
    "        tmp = data[data['case_length'] >= nr_events].groupby(case_id_col).head(nr_events)\n",
    "        tmp[\"orig_case_id\"] = tmp[case_id_col]\n",
    "        tmp[case_id_col] = tmp[case_id_col].apply(lambda x: \"%s_%s\" % (x, nr_events))\n",
    "        tmp[\"prefix_nr\"] = nr_events\n",
    "        dt_prefixes = pd.concat([dt_prefixes, tmp], axis=0)\n",
    "    dt_prefixes['case_length'] = dt_prefixes['case_length'].apply(lambda x: min(max_length, x))\n",
    "    return dt_prefixes\n",
    "\n",
    "def split_data_strict(data, train_ratio):\n",
    "    # split into train and test using temporal split and discard events that overlap the periods\n",
    "    data = data.sort_values(sorting_cols, ascending=True, kind='mergesort')\n",
    "    grouped = data.groupby(case_id_col)\n",
    "    start_timestamps = grouped[timestamp_col].min().reset_index()\n",
    "    start_timestamps = start_timestamps.sort_values(timestamp_col, ascending=True, kind='mergesort')\n",
    "    train_ids = list(start_timestamps[case_id_col])[:int(train_ratio*len(start_timestamps))]\n",
    "    train = data[data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort')\n",
    "    test = data[~data[case_id_col].isin(train_ids)].sort_values(sorting_cols, ascending=True, kind='mergesort')\n",
    "    split_ts = test[timestamp_col].min()\n",
    "    train = train[train[timestamp_col] < split_ts]\n",
    "    return (train, test)\n",
    "\n",
    "def get_label_numeric(data, bins, labels):\n",
    "    y = data.groupby(case_id_col).first()[label_col]  # one row per case\n",
    "    \n",
    "    label_mapping = {}\n",
    "    for idx, (lower, upper) in enumerate(zip(bins, bins[1:])):\n",
    "        label = labels[idx]\n",
    "        label_mapping[label] = idx\n",
    "    \n",
    "    return [label_mapping[label] for label in y.values.tolist()]\n",
    "\n",
    "\n",
    "def groupby_pad_all(train, test, val, cols, activity_col):\n",
    "    activity_train, label_lists_train = groupby_pad(train, cols, activity_col)\n",
    "    activity_test,label_lists_test = groupby_pad(test, cols, activity_col)\n",
    "    activity_val, label_lists_val = groupby_pad(val, cols, activity_col)\n",
    "    return activity_train, activity_test, activity_val, label_lists_train, label_lists_test, label_lists_val\n",
    "\n",
    "def groupby_pad(prefixes, cols, activity_col):\n",
    "    ans_act, label_lists = groupby_caseID(prefixes, cols, activity_col)\n",
    "    ######ACTIVITY########\n",
    "    activity = pad_data(ans_act)\n",
    "    return activity, label_lists\n",
    "\n",
    "def pad_data(data):\n",
    "    data[0] = nn.ConstantPad1d((0, max_prefix_length - data[0].shape[0]), 0)(data[0])\n",
    "    padding = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    return padding\n",
    "\n",
    "def groupby_caseID(data, cols, col):\n",
    "    groups = data[cols].groupby(case_id_col, as_index=True)\n",
    "    #case_ids = groups.groups.keys()\n",
    "    ans = [torch.tensor(list(y[col])) for _, y in groups]\n",
    "    label_lists = [y[label_col].iloc[0] for _, y in groups]\n",
    "    return ans, label_lists\n",
    "\n",
    "def groupby_pad_all_num(train, test, val, cols, numerical_features_col):\n",
    "    numerical_features_train = groupby_pad_num(train, cols, numerical_features_col)\n",
    "    numerical_features_test = groupby_pad_num(test, cols, numerical_features_col)\n",
    "    numerical_features_val = groupby_pad_num(val, cols, numerical_features_col)\n",
    "    return numerical_features_train, numerical_features_test, numerical_features_val\n",
    "\n",
    "def pad_data_num(data, max_prefix_length):\n",
    "    padded_data = [nn.ConstantPad2d((0, 0, 0, max_prefix_length - seq.shape[0]), 0)(seq) for seq in data]\n",
    "    padding = torch.stack(padded_data, dim=0)\n",
    "    return padding\n",
    "\n",
    "def groupby_pad_num(prefixes, cols, numerical_features_col):\n",
    "    ans_num = groupby_caseID_num(prefixes, cols, numerical_features_col)\n",
    "    numerical_features = pad_data_num(ans_num, max_prefix_length)  # Pass max_prefix_length here\n",
    "    return numerical_features\n",
    "\n",
    "def groupby_caseID_num(data, cols, numerical_features_col):\n",
    "    groups = data.groupby(case_id_col, as_index=True)\n",
    "    ans_num = [torch.tensor(y[numerical_features_col].values, dtype=torch.long) for _, y in groups]\n",
    "    return ans_num\n",
    "\n",
    "def create_index(log_df, column):\n",
    "    \"\"\"Creates an idx for a categorical attribute.\n",
    "    Args:\n",
    "        log_df: dataframe.\n",
    "        column: column name.\n",
    "    Returns:\n",
    "        index of a categorical attribute pairs.\n",
    "    \"\"\"\n",
    "    temp_list = temp_list = log_df[log_df[column] != 'none'][[column]].values.tolist()  # remove all 'none' values from the index\n",
    "    subsec_set = {(x[0]) for x in temp_list}\n",
    "    subsec_set = sorted(list(subsec_set))\n",
    "    alias = dict()\n",
    "    for i, _ in enumerate(subsec_set):\n",
    "        alias[subsec_set[i]] = i\n",
    "    # reorder by the index value\n",
    "    alias = {k: v for k, v in sorted(alias.items(), key=lambda item: item[1])}\n",
    "    return alias\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.tensor(np.eye(num_classes, dtype='uint8')[y])\n",
    "\n",
    "def create_indexes(i, data):\n",
    "    dyn_index = create_index(data, i)\n",
    "    index_dyn = {v: k for k, v in dyn_index.items()}\n",
    "    dyn_weights = to_categorical(sorted(index_dyn.keys()), len(dyn_index))\n",
    "    no_cols = len(data.groupby([i]))\n",
    "    return dyn_weights,  dyn_index, index_dyn, no_cols\n",
    "\n",
    "def prepare_inputs(X_train, X_test):\n",
    "        global ce\n",
    "        ce = ColumnEncoder()\n",
    "        X_train, X_test = X_train.astype(str), X_test.astype(str)\n",
    "        X_train_enc = ce.fit_transform(X_train)\n",
    "        X_test_enc = ce.transform(X_test)\n",
    "        return X_train_enc, X_test_enc, ce\n",
    "\n",
    "# https://towardsdatascience.com/using-neural-networks-with-embedding-layers-to-encode-high-cardinality-categorical-variables-c1b872033ba2\n",
    "class ColumnEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "        self.maps = dict()\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            # encode value x of col via dict entry self.maps[col][x]+1 if present, otherwise 0\n",
    "            X_copy.loc[:, col] = X_copy.loc[:, col].apply(lambda x: self.maps[col].get(x, -1)+1)\n",
    "        return X_copy\n",
    "    \n",
    "    def get_maps(self):\n",
    "        return self.maps\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            values = list(self.maps[col].keys())\n",
    "            # find value in ordered list and map out of range values to None\n",
    "            X_copy.loc[:, col] = [values[i-1] if 0 < i <= len(values) else None for i in X_copy[col]]\n",
    "        return X_copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # only apply to string type columns\n",
    "        self.columns = [col for col in X.columns if is_string_dtype(X[col])]\n",
    "        for col in self.columns:\n",
    "            self.maps[col] = OrderedDict({value: num for num, value in enumerate(sorted(set(X[col])))})\n",
    "        return self\n",
    "  \n",
    "def to_categorical_all(train, test, val, num_classes):\n",
    "        \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "        train_OHE = torch.tensor(np.eye(num_classes)[train])\n",
    "        test_OHE = torch.tensor(np.eye(num_classes)[test])\n",
    "        val_OHE = torch.tensor(np.eye(num_classes)[val])\n",
    "        return train_OHE, test_OHE, val_OHE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = \"CaseID\"\n",
    "activity_col = \"ActivityID\"\n",
    "timestamp_col = \"CompleteTimestamp\"\n",
    "label_col = \"Pump_Adjustment_Bin\"\n",
    "sorting_cols = [timestamp_col, activity_col]\n",
    "cat_cols = [activity_col]\n",
    "case_id_col = \"CaseID\"\n",
    "activity_col = \"ActivityID\"\n",
    "timestamp_col = \"CompleteTimestamp\"\n",
    "\n",
    "min_prefix_length = 1\n",
    "max_prefix_length = 132\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "dropout = 0.1\n",
    "lstm_size = 100\n",
    "num_classes = 3\n",
    "epochs = 50\n",
    "\n",
    "# Define the bin edges and labels\n",
    "bins = [0,10,25,90]\n",
    "labels = ['few', 'medium', 'many']\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#read in original data\n",
    "df = pd.read_csv('data/event_data_context_alex2.csv', index_col =\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseID</th>\n",
       "      <th>CompleteTimestamp</th>\n",
       "      <th>Vessel</th>\n",
       "      <th>ActivityID</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>event_id</th>\n",
       "      <th>Filter 1 DeltaP_mean</th>\n",
       "      <th>Filter 1 DeltaP_ema</th>\n",
       "      <th>Filter 1 DeltaP_max</th>\n",
       "      <th>Filter 1 DeltaP_min</th>\n",
       "      <th>...</th>\n",
       "      <th>Pump Circulation Flow_std</th>\n",
       "      <th>Pump Circulation Flow_sum</th>\n",
       "      <th>Pump Circulation Flow_dif</th>\n",
       "      <th>Tank Pressure_mean</th>\n",
       "      <th>Tank Pressure_ema</th>\n",
       "      <th>Tank Pressure_max</th>\n",
       "      <th>Tank Pressure_min</th>\n",
       "      <th>Tank Pressure_std</th>\n",
       "      <th>Tank Pressure_sum</th>\n",
       "      <th>Tank Pressure_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20K12001E</td>\n",
       "      <td>2020-10-14 13:38:51</td>\n",
       "      <td>R501</td>\n",
       "      <td>Filters</td>\n",
       "      <td>complete</td>\n",
       "      <td>2</td>\n",
       "      <td>0.049914</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>0.373810</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099007</td>\n",
       "      <td>100.634529</td>\n",
       "      <td>-0.025133</td>\n",
       "      <td>0.140960</td>\n",
       "      <td>0.222641</td>\n",
       "      <td>0.296219</td>\n",
       "      <td>0.005969</td>\n",
       "      <td>0.093770</td>\n",
       "      <td>320.684044</td>\n",
       "      <td>0.014511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20K12001E</td>\n",
       "      <td>2020-10-14 16:43:32</td>\n",
       "      <td>R501</td>\n",
       "      <td>1Pass_Prewet</td>\n",
       "      <td>complete</td>\n",
       "      <td>3</td>\n",
       "      <td>0.335981</td>\n",
       "      <td>0.045441</td>\n",
       "      <td>0.950388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274353</td>\n",
       "      <td>4971.595623</td>\n",
       "      <td>0.082580</td>\n",
       "      <td>0.671880</td>\n",
       "      <td>0.216346</td>\n",
       "      <td>0.944405</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.359341</td>\n",
       "      <td>8366.920742</td>\n",
       "      <td>-0.165722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20K12001E</td>\n",
       "      <td>2020-10-14 18:20:57</td>\n",
       "      <td>R501</td>\n",
       "      <td>Pump adjustment</td>\n",
       "      <td>complete</td>\n",
       "      <td>5</td>\n",
       "      <td>0.046075</td>\n",
       "      <td>0.045479</td>\n",
       "      <td>0.050554</td>\n",
       "      <td>0.044231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116169</td>\n",
       "      <td>0.216196</td>\n",
       "      <td>0.285838</td>\n",
       "      <td>0.033318</td>\n",
       "      <td>0.085035</td>\n",
       "      <td>34.966731</td>\n",
       "      <td>-0.182889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20K12001E</td>\n",
       "      <td>2020-10-14 18:31:04</td>\n",
       "      <td>R501</td>\n",
       "      <td>Pump stop</td>\n",
       "      <td>complete</td>\n",
       "      <td>6</td>\n",
       "      <td>0.389302</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.991595</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368901</td>\n",
       "      <td>137.490550</td>\n",
       "      <td>0.472329</td>\n",
       "      <td>0.578629</td>\n",
       "      <td>0.217301</td>\n",
       "      <td>0.639907</td>\n",
       "      <td>0.217287</td>\n",
       "      <td>0.139646</td>\n",
       "      <td>174.167334</td>\n",
       "      <td>0.417988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20K12001E</td>\n",
       "      <td>2020-10-14 18:32:41</td>\n",
       "      <td>R501</td>\n",
       "      <td>Pump adjustment</td>\n",
       "      <td>complete</td>\n",
       "      <td>7</td>\n",
       "      <td>0.381825</td>\n",
       "      <td>0.032229</td>\n",
       "      <td>0.991595</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362463</td>\n",
       "      <td>131.129489</td>\n",
       "      <td>-0.155598</td>\n",
       "      <td>0.444962</td>\n",
       "      <td>0.221960</td>\n",
       "      <td>0.639907</td>\n",
       "      <td>0.217287</td>\n",
       "      <td>0.205316</td>\n",
       "      <td>133.933439</td>\n",
       "      <td>0.410252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CaseID    CompleteTimestamp Vessel       ActivityID  \\\n",
       "2  20K12001E  2020-10-14 13:38:51   R501          Filters   \n",
       "3  20K12001E  2020-10-14 16:43:32   R501     1Pass_Prewet   \n",
       "5  20K12001E  2020-10-14 18:20:57   R501  Pump adjustment   \n",
       "6  20K12001E  2020-10-14 18:31:04   R501        Pump stop   \n",
       "7  20K12001E  2020-10-14 18:32:41   R501  Pump adjustment   \n",
       "\n",
       "  lifecycle:transition  event_id  Filter 1 DeltaP_mean  Filter 1 DeltaP_ema  \\\n",
       "2             complete         2              0.049914             0.050471   \n",
       "3             complete         3              0.335981             0.045441   \n",
       "5             complete         5              0.046075             0.045479   \n",
       "6             complete         6              0.389302             0.046063   \n",
       "7             complete         7              0.381825             0.032229   \n",
       "\n",
       "   Filter 1 DeltaP_max  Filter 1 DeltaP_min  ...  Pump Circulation Flow_std  \\\n",
       "2             0.373810             0.040615  ...                   0.099007   \n",
       "3             0.950388             0.000000  ...                   0.274353   \n",
       "5             0.050554             0.044231  ...                   0.000000   \n",
       "6             0.991595             0.019930  ...                   0.368901   \n",
       "7             0.991595             0.019930  ...                   0.362463   \n",
       "\n",
       "   Pump Circulation Flow_sum  Pump Circulation Flow_dif  Tank Pressure_mean  \\\n",
       "2                 100.634529                  -0.025133            0.140960   \n",
       "3                4971.595623                   0.082580            0.671880   \n",
       "5                   0.000000                   0.000000            0.116169   \n",
       "6                 137.490550                   0.472329            0.578629   \n",
       "7                 131.129489                  -0.155598            0.444962   \n",
       "\n",
       "   Tank Pressure_ema  Tank Pressure_max  Tank Pressure_min  Tank Pressure_std  \\\n",
       "2           0.222641           0.296219           0.005969           0.093770   \n",
       "3           0.216346           0.944405           0.003246           0.359341   \n",
       "5           0.216196           0.285838           0.033318           0.085035   \n",
       "6           0.217301           0.639907           0.217287           0.139646   \n",
       "7           0.221960           0.639907           0.217287           0.205316   \n",
       "\n",
       "   Tank Pressure_sum  Tank Pressure_dif  \n",
       "2         320.684044           0.014511  \n",
       "3        8366.920742          -0.165722  \n",
       "5          34.966731          -0.182889  \n",
       "6         174.167334           0.417988  \n",
       "7         133.933439           0.410252  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
